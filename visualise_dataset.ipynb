{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3556464e",
   "metadata": {},
   "source": [
    "# Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48fc40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8402dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/DATATWO/users/mincut/Object-Centric-VideoAnswering'\n",
    "datadir='/DATATWO/users/mincut/Object-Centric-VideoAnswering/data/train' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53691be",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=json.load(open(os.path.join(datadir,'train.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4866f6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m categories\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      5\u001b[0m         categories[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question_type'"
     ]
    }
   ],
   "source": [
    "categories = dict()\n",
    "for pt in f:\n",
    "    key = pt[\"question_type\"]\n",
    "    if key in categories.keys():\n",
    "        categories[key] += 1\n",
    "    else:\n",
    "        categories[key] = 1\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a367c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e256be41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[1]['scene_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30142c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_00001.mp4\n"
     ]
    }
   ],
   "source": [
    "print(f[1]['video_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92133e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the color of the object that exits the scene? descriptive\n",
      "What material is the object to collide with the red object? descriptive\n",
      "What is the material of the object to collide with the cube? descriptive\n",
      "How many gray spheres exit the scene after the cylinder enters the scene? descriptive\n",
      "How many rubber objects exit the scene? descriptive\n",
      "Are there any cylinders that exit the scene before the gray sphere exits the scene? descriptive\n",
      "What color is the stationary rubber object when the video begins? descriptive\n",
      "What material is the moving sphere when the video ends? descriptive\n",
      "How many moving metal objects are there when the video ends? descriptive\n",
      "How many moving cylinders are there? descriptive\n",
      "Are there any moving metal objects when the video ends? descriptive\n",
      "Which of the following is responsible for the gray object's exit? explanatory\n",
      "Which of the following is not responsible for the purple sphere's colliding with the gray object? explanatory\n",
      "What will happen next? predictive\n",
      "Without the cube, which event will happen? counterfactual\n",
      "What will not happen without the cylinder? counterfactual\n"
     ]
    }
   ],
   "source": [
    "for x in ((f[1]['questions'])):\n",
    "    print(x['question'],x['question_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc442a03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"video_00000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"video_00000.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efdf349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 dict_keys(['question_id', 'question', 'question_type', 'question_subtype', 'program', 'answer'])\n"
     ]
    }
   ],
   "source": [
    "import os, json \n",
    "root='/DATATWO/users/mincut/Object-Centric-VideoAnswering/data'\n",
    "questions_path = os.path.join(root,\"train\",f\"train.json\")\n",
    "dtst = json.load(open(questions_path))\n",
    "print(len(dtst),dtst[0]['questions'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee3c33b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4140473/2163411507.py\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      7 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdt_pt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'questions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m            \u001b[0mcat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(q.keys())\n",
      "dict_keys(['question_id', 'question', 'question_type', 'question_subtype', 'program', 'answer'])\n",
      "ipdb> dt_pt.keys()\n",
      "dict_keys(['questions', 'scene_index', 'video_filename'])\n",
      "ipdb> dt_pt['video_filename']\n",
      "'video_00000.mp4'\n",
      "ipdb> exit \n"
     ]
    }
   ],
   "source": [
    "root='/DATATWO/users/mincut/Object-Centric-VideoAnswering/data'\n",
    "for mode in ['train','validation','test']:\n",
    "    questions_path = os.path.join(root,mode,f\"{mode}.json\")\n",
    "    cat = dict()\n",
    "    dtst = json.load(open(questions_path))\n",
    "    for dt_pt in dtst:\n",
    "        for q in dt_pt['questions']:\n",
    "            import pdb ; pdb.set_trace()\n",
    "            cat[q['question_type']] = cat.get(q['question_type'],0)+1\n",
    "            print(q['question_id'])\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4febd8",
   "metadata": {},
   "source": [
    "# Preprocessing, Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f1565a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##def preprocess\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Open the video file\n",
    "\n",
    "def preprocess_video(path):\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    # Create a directory to store the extracted frames\n",
    "    name = path.split('/')[-1]\n",
    "    name = name.split('.')[0].split('_')[-1]\n",
    "    mode = path.split('/')[-4]\n",
    "    \n",
    "    output_dir = f'extracted_frames/{mode}/{name}'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Set the time interval to extract frames (in milliseconds)\n",
    "    interval = 250\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate the number of frames to skip\n",
    "    skip_frames = int(fps * (interval / 1000))\n",
    "\n",
    "    # Initialize variables\n",
    "    frame_count = 0\n",
    "    success = True\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while success:\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "        # Check if the frame was read successfully\n",
    "        if success:\n",
    "            # Check if the current frame is a multiple of the skip frames\n",
    "            if frame_count % skip_frames == 0:\n",
    "                # Save the extracted frame as an image file\n",
    "                output_path = os.path.join(output_dir, f\"frame{frame_count}.jpg\")\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                \n",
    "\n",
    "            # Increment the frame count\n",
    "            frame_count += 1\n",
    "\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada3343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87d44d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_video('data/train/video/video_00000-01000/video_00000.mp4')\n",
    "import glob\n",
    "path = 'data/'\n",
    "videos = [f for f in glob.glob(path + \"**/*.mp4\", recursive=True)]\n",
    "for video in videos:\n",
    "    preprocess_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e733b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d52af0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b2a2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/DATATWO/users/mincut/Object-Centric-VideoAnswering/data'\n",
    "import os, sys\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ddb7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VidQA(Dataset):\n",
    "    \n",
    "    def __init__(self,mode='train'):\n",
    "        \n",
    "        self.dtst_fldr = os.path.join(data,mode)\n",
    "        self.mode = mode\n",
    "        self.ques_file = os.path.join(self.dtst_fldr,f\"{mode}.json\")\n",
    "        self.ques,self.ques_types,self.ques_vid = self.read_questions()\n",
    "#         self.frames = self.read_frames()\n",
    "    \n",
    "    def read_frames(self,filename):\n",
    "        folder = os.path.join(root,'extracted_frames',mode)\n",
    "        idx = filename.split('_')[-1]\n",
    "        frame_fldr = os.path.join(folder,idx)\n",
    "        frm_seq = []\n",
    "        for fl in os.scandir(frame_fldr):\n",
    "            img = cv2.imread(fl.path)\n",
    "            # Convert the image from BGR to RGB format\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Convert image to torch tensor\n",
    "            img = torch.from_numpy(img.transpose((2, 0, 1)))\n",
    "            frm_seq.append(img)\n",
    "                        \n",
    "        return torch.stack(frm_seq)\n",
    "        \n",
    "    def read_questions(self):\n",
    "        mode = self.mode \n",
    "        q_path = os.path.join(root,mode,f\"{mode}.json\")\n",
    "        cat = dict()\n",
    "        q_list = []\n",
    "        qt_list = []\n",
    "        qt_vid = []\n",
    "        dtst = json.load(open(q_path))\n",
    "        for dt_pt in dtst:\n",
    "            vid_idx = dt_pt['video_filename'].split('.')[0].split('_')[-1]\n",
    "            vid_idx = int(vid_idx)\n",
    "            for q in dt_pt['questions']:\n",
    "                cat[q['question_type']] = cat.get(q['question_type'],0)+1\n",
    "                q_list.append(q['question'])\n",
    "                qt_list.append(q['question_id'])\n",
    "                qt_vid.append(vid_idx)\n",
    "                \n",
    "        return q_list,qt_list,qt_vid \n",
    "    \n",
    "    def gen_vocab(self):\n",
    "        pass \n",
    "        \n",
    "    def tokenize_sentences(self):\n",
    "        pass \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ques)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        return frm \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69f9bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dtst = VidQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1353af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VidQA at 0x7fe8170a4460>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a80499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
